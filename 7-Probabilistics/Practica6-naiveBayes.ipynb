{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 6: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaluació i entregues\n",
    "\n",
    "### Entregues\n",
    "Les pràctiques es realitzarán els divendres de 15:00 a 17:00. Cada setmana presentarem nous mètodes i algorismes vistos a la classe de teoria.\n",
    "\n",
    "L'entrega, es realitzarà el següent **dijous abans de les 23:59**, és a dir, quasibé tota una setmana per a realitzar les tasques. Es pujará un .ipynb ja executat, **sense ZIP**, on no només hi hagi codi sinó també text explicatiu de gràfiques, resultats, i analisis. Tingueu en compte que no es valora tant la quantitat, sino la qualitat del que s'hi explica. Hem d'aprendre a extreure el gra de la palla i presentar-ho de forma correcta i concisa.\n",
    "\n",
    "\n",
    "### Avaluació\n",
    "Aquesta pràctica tractará els següents temes:\n",
    "\n",
    "* A. Ús Naive Bayes Sci-kit Learn (60%)\n",
    "* B. Implementació Naive Bayes (40%)\n",
    "\n",
    "Per descomptat, tots els experiments seguiran aplicant els coneixements apresos a les pràctiques anteriors, és a dir, aplicant una metodologia correcta de crosvalidació per tal de poder extreure conclusions vàlides.\n",
    "\n",
    "<a href=\"https://www.flickr.com/photos/mattbuck007/3676624894\"><img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/12/naive-bayes-classifier.jpg\" width=500></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducció\n",
    "\n",
    "Els models Naive Bayes són un grup d’algoritmes de classificació extremadament ràpids i senzills que sovint són adequats per a conjunts de dades de molt alta dimensió.\n",
    "\n",
    "Com que són tan ràpids i tenen tan pocs paràmetres, acaben sent molt útils com a ràpid baseline per a un problemes de classificació senzills.\n",
    "\n",
    "Aquesta secció se centrarà en una explicació intuïtiva de com funcionen els classificadors ingenus de Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificació bayesiana\n",
    "\n",
    "Els classificadors de Bayes ingenus es basen en el teorema de Bayes, que és una equació que descriu la relació de probabilitats condicionals de quantitats estadístiques.\n",
    "\n",
    "A la classificació bayesiana, ens interessa trobar la probabilitat d'una etiqueta donades algunes característiques observades, que podem escriure com $ P (y ~|~ X) $.\n",
    "\n",
    "El teorema de Bayes ens explica com expressar-ho en termes de quantitats que podem calcular més directament:\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}\n",
    "$$\n",
    "\n",
    "Utilitzant l'assumpció ingenua d'independencia condicional, podem simplificar la relació per tot $i$.\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}\n",
    "$$\n",
    "\n",
    "Ja que $P(x_1, \\dots, x_n)$ és constant donada una entrada, es pot utilitzar la següent simplificació, que será donarà un resultat proporcional al real, per tant, tindrem la mateixa sortida.\n",
    "\n",
    "$$ P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) $$\n",
    "\n",
    "La classificació final s'obtindrà:\n",
    "\n",
    "$$ \\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tingueu en compte que $P (y)$ també s’anomena **probabilitat de classe** i $P (x_i | y)$ s’anomena **probabilitat condicional**. \n",
    "\n",
    "Els diferents classificadors ingenus de Bayes es diferencien principalment pels supòsits que fan sobre la distribució de $P (x_i | y)$. \n",
    "\n",
    "Intentem aplicar la fórmula anterior manualment al nostre conjunt de dades meteorològiques. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple:\n",
    "\n",
    "Anem a veure un exemple [(extret d'aquesta web)](https://www.geeksforgeeks.org/naive-bayes-classifiers/). Considereu aquestes dades meteorològiques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataGolf = pd.read_csv('playgolf.csv', delimiter=';')\n",
    "display(dataGolf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haurem de fer algunes precomputacions al nostre conjunt de dades. Hem de trobar $P (x_i | y_j)$ per a cada $x_i$ a $X$ i $y_j$ a $y$. Tots aquests càlculs es mostren a les taules següents:\n",
    "\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/naive-bayes-classification.png\" width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada taula ha calculat $P(x_i | y_j)$ per cada $x_i$ en $X$ i $y_j$ en $y$. Per exemple, la probabilitat de jugar a golf donat que la temperatura és fresca, és a dir, $P(temperature = Cool | PlayGolf = Yes) = 3/9$.\n",
    "\n",
    "A més, hem de trobar les probabilitats de classe ($P (y)$). Per exemple, $P (PlayGolf = Yes) = 9/14$.\n",
    "\n",
    "Ja podem fer servir el classificador ingenu de Bayes. Per exemple, si avui tenim aquestes condicions climatològiques, jugarem al golf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_golf = ['Sunny', 'Hot', 'Normal', False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Yes|today) = \\frac{P(Outlook=Sunny|Yes) P(Temperature=Hot|Yes) P(Humidity=Normal|Yes) P(Wind=False|Yes) P(Yes)}{P(today)}$\n",
    "\n",
    "$P(No|today) = \\frac{P(Outlook=Sunny|No) P(Temperature=Hot|No) P(Humidity=Normal|No) P(Wind=False|No) P(No)}{P(today)}$\n",
    "\n",
    "Ignorem el $P(today)$ per què es comú a les dues probabilitats, i calculem:\n",
    "\n",
    "$$P(Yes|today) \\propto \\frac{2}{9}·\\frac{2}{9}·\\frac{6}{9}·\\frac{6}{9}·\\frac{9}{14} \\simeq 0.0141 $$\n",
    "\n",
    "$$P(No|today) \\propto \\frac{3}{5}·\\frac{2}{5}·\\frac{1}{5}·\\frac{2}{5}·\\frac{5}{14} \\simeq 0.0068 $$\n",
    "\n",
    "Podem normalitzar-ho per sumar 1, \n",
    "$$P(Yes|today) = \\frac{0.0141}{0.0068+0.0141}=0.67$$\n",
    "$$P(No|today) = \\frac{0.0068}{0.0068+0.0141}=0.33$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df_golf = pd.DataFrame([today_golf], columns = ['Outlook','Temperature', 'Humidity','Windy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "X_golf = dataGolf.drop(\"PlayGolf\", axis=\"columns\")\n",
    "y_golf = dataGolf[\"PlayGolf\"]\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(X_golf)\n",
    "X_golf_encoded = enc.transform(X_golf)\n",
    "\n",
    "model_cat = CategoricalNB(alpha=1)\n",
    "model_cat.fit(X_golf_encoded, y_golf)\n",
    "\n",
    "model_cat.predict_proba(enc.transform(today_df_golf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipus de Classificadors Naive Bayes\n",
    "Tot el que necessitem ara és algun model per calcular **$ P(x_i \\mid y) $** per a cada etiqueta, i n'existeixen varis depenent del tipus de dades. \n",
    "\n",
    "Podeu trobar l'explicació més precisa a la documentació, pero a continuació n'explicarem els trets fonamentals (Sabrieu dir quin model hem fet servir en l'exemple anterior?):\n",
    "\n",
    "Tingueu en compte que $\\alpha$ es un prior de suavització, i tenen en compte característiques no vistes durant l'aprenentatge. Impedeixen probabilitats nul·les en altres càlculs. Si $\\alpha = 1$, s’anomena suavització de Laplace, mentre que $\\alpha < 1$ s’anomena suavització de Lidstone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)\n",
    "Si tenim dades continues, i creiem que asumeixen una distribució normal, les podem modelitzar amb la mitja i la desviació estandar.\n",
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "\n",
    "#### [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
    "Es fa servir si tenim caracteristiques que compten occurencies. Per exemple, en text, si comptem quantes vegades apareix un terme en el document (una paraula). \n",
    "\n",
    "$$P(x_i \\mid y)\\sim \\hat{\\theta}_{yi} · x_i = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} · x_i$$\n",
    "\n",
    "#### [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes)\n",
    "Si les dades són binaries, millor utilitzar aquesta problabilitat.\n",
    "\n",
    "$$P(x_i \\mid y) = P(i \\mid y) · x_i + (1 - P(i \\mid y)) · (1 - x_i)$$\n",
    "\n",
    "#### [Categorical Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes)\n",
    "Si les dades són categoriques.\n",
    "$$P(x_i = t \\mid y = c ; \\alpha) = \\frac{ N_{tic} + \\alpha}{N_{c} + \\alpha n_i}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Es faràn servir varies bases de dades per comparar els mètodes més idonis depenent del tipus de dades:\n",
    "\n",
    "* [**playgolf**](https://medium.com/@hrishavkmr/naive-bayes-in-machine-learning-5c0972340b76) (db: `playgolf.csv`)\n",
    "* [**iris**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) (db: `datasets.load_iris`)\n",
    "* [**zoo**](https://www.kaggle.com/uciml/zoo-animal-classification) (db: `zoo_animal.csv`)\n",
    "* [**heart-disease-uci**](https://www.kaggle.com/ronitf/heart-disease-uci) (db: `heart.csv`)\n",
    "* [**20newsgroups**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) (db: `datasets.fetch_20newsgroups`) (Al appendix veure-ho com tranformar text en caracteristiques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Ús Naive Bayes Sci-kit Learn (60%)\n",
    "\n",
    "En aquest apartat us recomano que feu ús de funcions propies per a llegir una base de dades, aplicar-hi un model, fer crosvalidacio i que retorni els resultats. D'aquesta forma podreu reaprofitar molt de codi.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def load_dataset(which):\n",
    "    X, y = None, None\n",
    "    # si s'han de netejar o codificar les dades, feu-ho aqui directament\n",
    "    if which == \"iris\":\n",
    "        pass\n",
    "    elif which == \"playgolf\":\n",
    "        pass\n",
    "    elif which == \"zoo\":\n",
    "        pass\n",
    "    elif which == \"heart-disease-uci\":\n",
    "        pass\n",
    "    elif which == \"20newsgroups\":\n",
    "        pass\n",
    "    else:\n",
    "        raise (\"UNKNOWN DATASET\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def experiment(dataset_name, model, debug=True):\n",
    "    X, y = load_dataset(dataset_name)\n",
    "\n",
    "    if X is None or y is None:\n",
    "        return -1\n",
    "\n",
    "    if debug:\n",
    "        model.fit(X, y)\n",
    "        return model.score(X, y)\n",
    "    else:            \n",
    "        try:\n",
    "            model.fit(X, y)\n",
    "            return model.score(X, y)\n",
    "        except Exception as e:\n",
    "            print('\\033[91m'+\"ERROR {}\".format(e)+'\\x1b[0m')\n",
    "            return -100    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, CategoricalNB\n",
    "\n",
    "models = []\n",
    "datasets = []\n",
    "\n",
    "for d in datasets:\n",
    "    for m in models:\n",
    "        result = experiment(d, m)\n",
    "        print('{} + {:20} = {:.3f}'.format(d, str(m), result))\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instancia_model = GaussianNB()\n",
    "result = experiment('playgolf', instancia_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preguntes\n",
    "1. Carrega els diferents datasets i explica quin tipus de classificador de bayes creieu que serà més adient per les dades.\n",
    "\n",
    "2. Fés una taula mostrant el accuracy de cada model per cada un dels datasets. \n",
    "\n",
    "3. Mostra els resultats gràficament. Quin tipus de Bayes funciona millor per quin dataset. De mitjana, quin ho fa millor?\n",
    "\n",
    "4. Et sorprenen el resultats? Com és possible que certs models funcionin per dades que en principi no haurien de funcionar? Explica els atributs més importants de cada classificador. Mostra'n exemples i analitza'ls.\n",
    "\n",
    "5. Si el nostre dataset conté dades de diferent naturalesa, és a dir, uns atributs categorics, d'altres continus, d'altres binaris, creus que es podrien combinar els diferents classificadors de bayes? Seguint la formula original, com es faria?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Implementació Naive Bayes (40%)\n",
    "\n",
    "En aquest apartat implementarem un classificador Ingenu de Bayes per cada un dels tipus de dades. Per fer això, i que sigui fàcil de comparar amb els resultats del apartat anterior, feu servir un esquelet similar al següent:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.n_samples = 0\n",
    "        self.class_count_ = []  # numero de exemples de cada categoria\n",
    "        self.class_prior_ = []  # prior per cada categoria\n",
    "        self.classes_ = []      # nom o etiqueta de cada label\n",
    "        \n",
    "\n",
    "    def __str__(self):\n",
    "        # toString()  \n",
    "        return (self.__class__.__name__+\"()\")\n",
    "        \n",
    "    def calcular_conditional(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the conditional probability for the training data for each feature\n",
    "        \"\"\"\n",
    "        raise(\"TO BE DONE. Ho heu que implementar per cada subclasse\")\n",
    "    \n",
    "    def calcular_likelihood_log(self, X):\n",
    "        \"\"\"\n",
    "        Compute the unnormalized posterior log probability of X\n",
    "        \"\"\"\n",
    "        raise(\"TO BE DONE. Ho heu que implementar per cada subclasse\")\n",
    "\n",
    "    def calcular_prior(self, y):\n",
    "        \"\"\"\n",
    "        Compute the class prior\n",
    "        \"\"\"\n",
    "        raise(\"TO BE DONE.\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probs, axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probs = np.exp(self.predict_log_proba(X))\n",
    "        total_sum = np.sum(probs, axis=1)\n",
    "        for i in range(len(self.classes_)):\n",
    "            probs[:,i] /= total_sum\n",
    "        return probs\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        probs = self.calcular_likelihood_log(X)\n",
    "        for i in range(len(self.classes_)):\n",
    "            probs[:,i] += np.log(self.class_prior_[i])\n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        # calcular p(y)\n",
    "        self.calcular_prior(y)\n",
    "        \n",
    "        # per cada una de les columnes de x\n",
    "        for i in range(X.shape[1]):\n",
    "            # calcular p(xi | y)\n",
    "            self.calcular_conditional(X, y)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes(NaiveBayes):\n",
    "    def __init__(self, alpha=1):\n",
    "        super().__init__(alpha)\n",
    "        self.theta_ = None  # mean\n",
    "        self.sigma_ = None  # var\n",
    "        \n",
    "\n",
    "    def calcular_conditional(self, X, y):\n",
    "        # todo\n",
    "        # heu de calcular la mitjana i la variança per cada una de les categories y\n",
    "        \n",
    "        self.theta_ = np.zeros((len(self.classes_), X.shape[1]))\n",
    "        self.sigma_ = np.zeros((len(self.classes_), X.shape[1]))\n",
    "                                \n",
    "    def calcular_likelihood_log(self, X):\n",
    "        # aplicar la formula de com s'apropa X a les vostres theha i sigma..\n",
    "        # tip: recomenat utilitzar suma de logaritmes enlloc de productes per estabilitat\n",
    "        pass       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CategoricalNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNaiveBayes(NaiveBayes):\n",
    "    def __init__(self, alpha=1):\n",
    "        super().__init__(alpha)\n",
    "        self.category_count_ = []\n",
    "        self.feature_log_prob_ = []\n",
    "            \n",
    "    def calcular_conditional(self, X, y):\n",
    "        # todo\n",
    "        # contar quants cops apareix cada categoria\n",
    "        \n",
    "        # convertir els counts a probabilitats segons la formula\n",
    "        pass\n",
    "\n",
    "    def calcular_likelihood_log(self, X):\n",
    "        # todo\n",
    "        # sumar el logaritme de les probabilitats segons X  \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes(NaiveBayes):\n",
    "    def __init__(self, alpha=1):\n",
    "        super().__init__(alpha)\n",
    "        self.feature_log_prob_ = []\n",
    "\n",
    "    def calcular_conditional(self, X, y):\n",
    "        # todo\n",
    "        # calcular self.feature_log_prob_ segons la formula corresponent\n",
    "\n",
    "\n",
    "    def calcular_likelihood_log(self, X):\n",
    "        # todo\n",
    "        # dot product entre les features i les vostres probabilitats calculades     \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preguntes\n",
    "Per aquest apartat, intenteu de fer-ho en python directament, per a poder debugar el codi més fàcilment. Compareu el vostre resultat amb el de sklearn. Feu-ho també per passos intermitjos, i veure que ho esteu fent correctament.\n",
    "\n",
    "1. Implementa $P(x_i|y)$ per dades categoriques (Categorical Naive Bayes)\n",
    "2. Implementa $P(x_i|y)$ per dades continues (Gaussian Naive Bayes)\n",
    "3. Compara els resultats obtinguts amb els de sklearn. S'assemblen?\n",
    "4. (opcional) Implementa $P(x_i|y)$ per dades ordinals (Multinomial Naive Bayes) (+1pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "#### (no s'hi ha de fer res, tan sols es per si us ajuda a entendre-ho tot millor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/PDSH-cover-small.png?raw=1\">\n",
    "\n",
    "*Aquesta subsecció conté parts extretes de [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; i està disponible a [GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
    "\n",
    "Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem.\n",
    "\n",
    "This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Classification\n",
    "\n",
    "Naive Bayes classifiers are built on Bayesian classification methods.\n",
    "\n",
    "These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\n",
    "\n",
    "In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$.\n",
    "\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
    "$$\n",
    "\n",
    "If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\n",
    "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
    "$$\n",
    "\n",
    "All we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\n",
    "\n",
    "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "\n",
    "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "\n",
    "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
    "\n",
    "Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.\n",
    "In this classifier, the assumption is that *data from each label is drawn from a simple Gaussian distribution*.\n",
    "Imagine that you have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions.\n",
    "\n",
    "This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution.\n",
    "\n",
    "The result of this naive Gaussian assumption is shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses.\n",
    "\n",
    "With this generative model in place for each class, we have a simple recipe to compute the likelihood $P({\\rm features}~|~L_1)$ for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point.\n",
    "\n",
    "This procedure is implemented in Scikit-Learn's ``sklearn.naive_bayes.GaussianNB`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some new data and predict the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
    "ynew = model.predict(Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot this new data to get an idea of where the decision boundary is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()\n",
    "plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slightly curved boundary in the classifications—in general, the boundary in Gaussian naive Bayes is quadratic.\n",
    "\n",
    "A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the ``predict_proba`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yprob = model.predict_proba(Xnew)\n",
    "yprob[-8:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns give the posterior probabilities of the first and second label, respectively.\n",
    "If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a useful approach.\n",
    "\n",
    "Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results.\n",
    "Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.\n",
    "Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.\n",
    "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribution with a best-fit multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Classifying Text\n",
    "\n",
    "One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.\n",
    "We discussed the extraction of such features from text in [Feature Engineering](05.04-Feature-Engineering.ipynb); here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.\n",
    "\n",
    "Let's download the data and take a look at the target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity here, we will select just a few of these categories, and download the training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
    "             'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a representative entry from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the distributions of lengths of the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lens = [len(d) for d in train.data]\n",
    "\n",
    "def plot_loghist(x, bins):\n",
    "  hist, bins = np.histogram(x, bins=bins)\n",
    "  logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "  plt.hist(x, bins=logbins)\n",
    "  plt.xscale('log')\n",
    "\n",
    "plot_loghist(lens, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.\n",
    "For this we will use the TF-IDF vectorizer, and create a pipeline that attaches it to a multinomial naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this pipeline, we can apply the model to the training data, and predict labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.\n",
    "For example, here is the confusion matrix between the true and predicted labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "plt.figure(figsize=(20,14))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity.\n",
    "This is perhaps an expected area of confusion!\n",
    "\n",
    "The very cool thing here is that we now have the tools to determine the category for *any* string, using the ``predict()`` method of this pipeline.\n",
    "Here's a quick utility function that will return the prediction for a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.target, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_category('sending a payload to the ISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_category('sending bytes to the network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_category('sending chocolate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_category('discussing islam vs atheism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_category('determining the screen resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking.\n",
    "Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Naive Bayes\n",
    "\n",
    "Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model.\n",
    "That said, they have several advantages:\n",
    "\n",
    "- They are extremely fast for both training and prediction\n",
    "- They provide straightforward probabilistic prediction\n",
    "- They are often very easily interpretable\n",
    "- They have very few (if any) tunable parameters\n",
    "\n",
    "These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification.\n",
    "If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.\n",
    "If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
    "\n",
    "Naive Bayes classifiers tend to perform especially well in one of the following situations:\n",
    "\n",
    "- When the naive assumptions actually match the data (very rare in practice)\n",
    "- For very well-separated categories, when model complexity is less important\n",
    "- For very high-dimensional data, when model complexity is less important\n",
    "\n",
    "The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in *every single dimension* to be close overall).\n",
    "This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information.\n",
    "For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
